{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from  torch.nn import functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 嵌入层表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入\n",
    "\n",
    "class PositionEncoder(nn.Module):\n",
    "    def __init__(self,d_model,max_seq_len=80):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "\n",
    "\n",
    "        pe=torch.zeros(max_seq_len,d_model)\n",
    "        #初始化位置编码0\n",
    "\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos,i]=math.sin(pos / (10000**((2*i)/d_model)))\n",
    "                pe[pos,i+1]=math.cos(pos / (10000**((2*(i+1))/d_model)))\n",
    "        #位置编码\n",
    "        \n",
    "        pe=pe.unsqueeze(0)\n",
    "        #加一个维度，因为数据一般会以（batch_size,seq_len,d_model)进入，方便广播机制\n",
    "        self.register_buffer('pe',pe)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=x*math.sqrt(self.d_model)\n",
    "        #增大原有词向量，减少位置编码对原有语义的影响\n",
    "\n",
    "        seq_len=x.size(1)\n",
    "        #第二个维度的长度\n",
    "        x=x+torch.tensor(self.pe[:,:seq_len],requires_grad=False).cuda()\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入head、d_model、dropout、q k v \n",
    "\n",
    "class MultiHeadAtention(nn.Module):\n",
    "\n",
    "    def __init__(self,heads,d_model,dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model=d_model\n",
    "        self.d_k=d_model//heads\n",
    "        self.h=heads\n",
    "\n",
    "        self.q_linear=nn.Linear(d_model,d_model)\n",
    "        self.k_linear=nn.Linear(d_model,d_model)\n",
    "        self.v_linear=nn.Linear(d_model,d_model)\n",
    "        #将嵌入向量线性变换为q、k、v。你可能会好奇为什么是d_model，而不是d_k。因为后面会拆分为多头的\n",
    "\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        self.out=nn.Linear(d_model,d_model)\n",
    "\n",
    "\n",
    "    def attention(q,k,v,d_k,mask=None,dropout=None):\n",
    "        scores=torch.matmul(q,k.transpose(-2,-1)/math.sqrt(d_k))\n",
    "#         维度（batch_size,heads,seq_len,d_k）\n",
    "\n",
    "        if mask is not None:\n",
    "            mask=mask.unsqueeze(1)\n",
    "            scores=scores.masked_fill(mask==0,-1e9)\n",
    "\n",
    "        scores=F.softmax(scores,dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores=dropout(scores)\n",
    "\n",
    "        output=torch.matmul(scores,v)\n",
    "        #（batch_size,heads,seq_len,d_k）\n",
    "        return output\n",
    "\n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        #输入序列（batch_size,seq_len,d_model）\n",
    "\n",
    "        bs=q.size(0)\n",
    "        #确定batch_size\n",
    "\n",
    "        k=self.k_linear(k).view(bs,-1,self.h,self.d_k) \n",
    "        q=self.q_linear(q).view(bs,-1,self.h,self.d_k)\n",
    "        v=self.v_linear(v).view(bs,-1,self.h,self.d_k)\n",
    "        #经过qkv变换，拆分多头，序列（batch_size,seq_len,heads,d_k）\n",
    "\n",
    "        q=q.transpose(1,2)\n",
    "        k=k.transpose(1,2)\n",
    "        v=v.transpose(1,2)\n",
    "        #序列（batch_size,heads,seq_len,d_k）转化为多头的形式，方便计算attention\n",
    "\n",
    "        scores=self.attention(q,k,v,self.d_k,mask,self.dropout)\n",
    "        #序列（batch_size,heads,seq_len,d_k）维度不变，但是d_k部分的数值会经历softmax\n",
    "\n",
    "        concat=scores.transpose(1,2).contiguous().view(bs,-1,self.d_model)\n",
    "        #转化为序列（batch_size,seq_len,heads,d_k），方便多头合并。\n",
    "        #再进行重排，得到序列（batch_size,seq_len,d_model）\n",
    "\n",
    "        output=self.out(concat)\n",
    "        #（batch_size,seq_len,d_model）经历线性层，最终的输出意义是注意力值，且维度输入与输出一致！\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前馈层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入d_model、d_ff、dropout\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff=2048,dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1=nn.Linear(d_model,d_ff)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.linear_2=nn.Linear(d_ff,d_model)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.dropout(F.relu(self.linear_1(x)))\n",
    "        x=self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "    #线性层 + relu + dropout +  线性层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入d_model\n",
    "\n",
    "class NormLayer(nn.Module):\n",
    "    def __init__(self,d_model,eps=1e-6):\n",
    "        super().__init___()\n",
    "\n",
    "        self.size=d_model\n",
    "        self.alpha=nn.Parameter(torch.ones(self.size))\n",
    "        self.bias=nn.Parameter(torch.ones(self.size))\n",
    "        self.eps=eps\n",
    "\n",
    "    def forward(self,x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,heads,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1=NormLayer(d_model)\n",
    "        self.norm_2=NormLayer(d_model)\n",
    "        self.attn=MultiHeadAtention(heads,d_model,dropout=dropout)\n",
    "        self.ff=FeedForward(d_model,dropout=dropout)\n",
    "        self.dropout_1=nn.Dropout(dropout)\n",
    "        self.dropout_2=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x,mask):\n",
    "        x2=self.norm_1(x)\n",
    "        x=x+self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x=self.norm_2(x)\n",
    "        x=x+self.dropout_2(self.ff(x2))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,N,heads,dropout):\n",
    "        super().__init__()\n",
    "        self.N=N\n",
    "        self.embed=None\n",
    "        self.pe=PositionEncoder(d_model,dropout=dropout)\n",
    "        self.layers=get_clones(EncoderLayer(d_model,heads,dropout),N)\n",
    "        self.norm=NormLayer(d_model)\n",
    "    \n",
    "    def forward(self,src,mask):\n",
    "        x=self.embed(src)\n",
    "        x=self.pe(x)\n",
    "        for i in  range(self.N):\n",
    "            x=self.layers[i](x,mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,N,heads,dropout):\n",
    "        super().__init__()\n",
    "        self.norm_1=NormLayer(d_model)\n",
    "        self.norm_2=NormLayer(d_model)\n",
    "        self.norm_3=NormLayer(d_model)\n",
    "\n",
    "        self.dropout_1=nn.Dropout(dropout)\n",
    "        self.dropout_2=nn.Dropout(dropout)\n",
    "        self.dropout_3=nn.Dropout(dropout)\n",
    "\n",
    "        self.attn_1=MultiHeadAtention(heads,d_model,dropout=dropout)\n",
    "        self.attn_2=MultiHeadAtention(heads,d_model,dropout=dropout)\n",
    "\n",
    "        self.ff=FeedForward(d_model,dropout=dropout)\n",
    "\n",
    "    def forward(self,x,e_outputs,src_mask,tar_mask):\n",
    "        x2=self.norm_1(x)\n",
    "        x=x+self.dropout_1(self.attn_1(x2,x2,x2,tar_mask))\n",
    "        x2=self.norm_2(x)\n",
    "        x=x+self.dropout_2(self.attn_2(x2,e_outputs,e_outputs,src_mask))\n",
    "        x2=self.norm_3(x)\n",
    "        x=x+self.dropout_3(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N=N\n",
    "        self.embed=None\n",
    "        self.pe=PositionEncoder(d_model,dropout=dropout)\n",
    "        self.layers=get_clones(DecoderLayer(d_model,heads,dropout),N)\n",
    "        self.norm=NormLayer(d_model)\n",
    "        \n",
    "    def forward(self,trg,e_outputs,src_mask,trg_mask):\n",
    "        x=self.embed(trg)\n",
    "        x=self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x=self.layers[i](x,e_outputs,src_mask,trg_mask)\n",
    "            return self.norm(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,src_vocab,trg_vocab,d_model,N,heads,dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder=Encoder(src_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder=Decoder(trg_vocab, d_model, N, heads, dropout)\n",
    "        self.out=nn.Linear(d_model, trg_vocab)\n",
    "\n",
    "    def forward(self,src,trg,src_mask,trg_mask):\n",
    "        e_outputs=self.encoder(src,src_mask)\n",
    "        d_output=self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output=self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型参数定义\n",
    "d_model = 512\n",
    "heads = 8\n",
    "N = 6\n",
    "\n",
    "src_vocab = len(EN_TEXT.vocab)\n",
    "trg_vocab = len(FR_TEXT.vocab)\n",
    "model = Transformer(src_vocab, trg_vocab, d_model, N, heads)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# 训练模型\n",
    "def train_model(epochs, print_every=100):\n",
    "    model.train()\n",
    "\n",
    "    start = time.time()\n",
    "    temp = start\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            src = batch.English.transpose(0, 1)\n",
    "            trg = batch.French.transpose(0, 1)\n",
    "\n",
    "            # The French sentence we input has all words except the last, as it is using each word to predict the next\n",
    "            trg_input = trg[:, :-1]\n",
    "\n",
    "            # The words we are trying to predict\n",
    "            targets = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            # Create function to make masks using mask code above\n",
    "            src_mask, trg_mask = create_masks(src, trg_input)\n",
    "\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), targets, ignore_index=target_pad)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.data[0]\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                loss_avg = total_loss / print_every\n",
    "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, %d s per %d iters\" % (\n",
    "                    (time.time() - start) // 60, epoch + 1, i + 1, loss_avg,\n",
    "                    (time.time() - start) // (i + 1), print_every))\n",
    "\n",
    "                total_loss = 0\n",
    "                temp = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
